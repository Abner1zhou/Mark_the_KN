{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/abner/Documents/PycharmProjects/Mark_the_KN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.transformer.model import Transformer\n",
    "from model.transformer.layers import create_padding_mask\n",
    "from model.transformer.train_helper import get_optimizer, get_loss, CustomSchedule\n",
    "from utils.params_utils import get_params\n",
    "from utils.metrics import micro_f1,macro_f1\n",
    "from utils.data_proc import data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_gpu(use_cpu=False):\n",
    "    if use_cpu:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = '1'  # gpu报错 使用cpu运行\n",
    "    else:\n",
    "        gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            try:\n",
    "                tf.config.experimental.set_virtual_device_configuration(\n",
    "                    gpus[0],\n",
    "                    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)])\n",
    "                logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "                print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "\n",
    "                # for gpu in gpus:\n",
    "                #     tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = get_params(\"transformer\")\n",
    "X_train, X_test, y_train, y_test, vocab, mlb = data_loader(params, is_rebuild_dataset=False)\n",
    "y_train = tf.constant(y_train, tf.float32)\n",
    "y_test = tf.constant(y_test, tf.float32)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据集缓存到内存中以加快读取速度。\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(params.buffer_size, \n",
    "                                      reshuffle_each_iteration=True).batch(params.batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset=test_dataset.batch(params.batch_size)\n",
    "# 流水线技术 重叠训练的预处理和模型训练步骤。当加速器正在执行训练步骤 N 时，CPU 开始准备步骤 N + 1 的数据。这样做可以将步骤时间减少到模型训练与抽取转换数据二者所需的最大时间（而不是二者时间总和）。\n",
    "# 没有流水线技术，CPU 和 GPU/TPU 大部分时间将处于闲置状态:\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(params.num_layers, params.d_model, params.num_heads, params.dff,\n",
    "                          params.input_vocab_size, params.output_dim,\n",
    "                          params.max_position_encoding,\n",
    "                          rate=params.dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_optimizer()\n",
    "train_loss, train_accuracy = get_loss()\n",
    "loss_function = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction='none')\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_manager = tf.train.CheckpointManager(ckpt, params.results_dir, max_to_keep=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.float32),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(inp, tar):\n",
    "\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = transformer(inp, True, enc_padding_mask=enc_padding_mask)\n",
    "        loss = loss_function(tar, predictions)\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar, predictions)\n",
    "\n",
    "    mi_f1 = micro_f1(tar, predictions)\n",
    "    ma_f1 = macro_f1(tar, predictions)\n",
    "    return mi_f1, ma_f1\n",
    "\n",
    "\n",
    "def predict(inp, tar, enc_padding_mask):\n",
    "    predictions = transformer(inp, False, enc_padding_mask=enc_padding_mask)\n",
    "    mi_f1 = micro_f1(tar, predictions)\n",
    "    ma_f1 = macro_f1(tar, predictions)\n",
    "    return mi_f1, ma_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.9060 micro_f1 0.1038 macro_f1 0.0615 val_micro_f1 0.1067 val_macro_f1 0.0578\n",
      "Epoch 1 Batch 50 Loss 0.3729 micro_f1 0.3730 macro_f1 0.0198 val_micro_f1 0.4260 val_macro_f1 0.0260\n",
      "Epoch 1 Batch 100 Loss 0.2768 micro_f1 0.3651 macro_f1 0.0241 val_micro_f1 0.4153 val_macro_f1 0.0273\n",
      "Epoch 1 Batch 150 Loss 0.2390 micro_f1 0.5284 macro_f1 0.0955 val_micro_f1 0.6175 val_macro_f1 0.1165\n",
      "Epoch 1 Batch 200 Loss 0.2131 micro_f1 0.5686 macro_f1 0.1712 val_micro_f1 0.6762 val_macro_f1 0.2255\n",
      "Epoch 1 Batch 250 Loss 0.1928 micro_f1 0.6521 macro_f1 0.2600 val_micro_f1 0.7393 val_macro_f1 0.4038\n",
      "Epoch 1 Loss 0.1828 Accuracy 0.9368\n",
      "Time taken for 1 epoch: 211.95725655555725 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.0912 micro_f1 0.7303 macro_f1 0.2666 val_micro_f1 0.7418 val_macro_f1 0.3973\n",
      "Epoch 2 Batch 50 Loss 0.0907 micro_f1 0.7723 macro_f1 0.4359 val_micro_f1 0.8193 val_macro_f1 0.5138\n",
      "Epoch 2 Batch 100 Loss 0.0842 micro_f1 0.7835 macro_f1 0.4281 val_micro_f1 0.8394 val_macro_f1 0.5456\n",
      "Epoch 2 Batch 150 Loss 0.0814 micro_f1 0.7910 macro_f1 0.3978 val_micro_f1 0.8357 val_macro_f1 0.5566\n",
      "Epoch 2 Batch 200 Loss 0.0779 micro_f1 0.8035 macro_f1 0.5541 val_micro_f1 0.8552 val_macro_f1 0.6044\n",
      "Epoch 2 Batch 250 Loss 0.0746 micro_f1 0.8308 macro_f1 0.4835 val_micro_f1 0.8531 val_macro_f1 0.5771\n",
      "Epoch 2 Loss 0.0732 Accuracy 0.9748\n",
      "Time taken for 1 epoch: 207.63846969604492 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.0440 micro_f1 0.8564 macro_f1 0.4819 val_micro_f1 0.8499 val_macro_f1 0.5722\n",
      "Epoch 3 Batch 50 Loss 0.0560 micro_f1 0.8414 macro_f1 0.5359 val_micro_f1 0.8616 val_macro_f1 0.6396\n",
      "Epoch 3 Batch 100 Loss 0.0534 micro_f1 0.8386 macro_f1 0.5848 val_micro_f1 0.8830 val_macro_f1 0.6384\n",
      "Epoch 3 Batch 150 Loss 0.0534 micro_f1 0.8235 macro_f1 0.4477 val_micro_f1 0.8862 val_macro_f1 0.5621\n",
      "Epoch 3 Batch 200 Loss 0.0522 micro_f1 0.8651 macro_f1 0.6564 val_micro_f1 0.8837 val_macro_f1 0.6538\n",
      "Epoch 3 Batch 250 Loss 0.0505 micro_f1 0.8693 macro_f1 0.5536 val_micro_f1 0.9117 val_macro_f1 0.6867\n",
      "Epoch 3 Loss 0.0500 Accuracy 0.9827\n",
      "Time taken for 1 epoch: 207.8992202281952 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.0372 micro_f1 0.8906 macro_f1 0.5492 val_micro_f1 0.8722 val_macro_f1 0.6286\n",
      "Epoch 4 Batch 50 Loss 0.0448 micro_f1 0.8689 macro_f1 0.6320 val_micro_f1 0.8904 val_macro_f1 0.6575\n",
      "Epoch 4 Batch 100 Loss 0.0429 micro_f1 0.9331 macro_f1 0.6962 val_micro_f1 0.9026 val_macro_f1 0.6766\n",
      "Epoch 4 Batch 150 Loss 0.0429 micro_f1 0.8972 macro_f1 0.5803 val_micro_f1 0.9191 val_macro_f1 0.6900\n",
      "Epoch 4 Batch 200 Loss 0.0423 micro_f1 0.8544 macro_f1 0.6024 val_micro_f1 0.8789 val_macro_f1 0.6743\n",
      "Epoch 4 Batch 250 Loss 0.0417 micro_f1 0.8989 macro_f1 0.6617 val_micro_f1 0.9012 val_macro_f1 0.7060\n",
      "Epoch 4 Loss 0.0409 Accuracy 0.9862\n",
      "Time taken for 1 epoch: 208.0118579864502 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.0338 micro_f1 0.8863 macro_f1 0.6046 val_micro_f1 0.8834 val_macro_f1 0.6362\n",
      "Epoch 5 Batch 50 Loss 0.0383 micro_f1 0.9071 macro_f1 0.6428 val_micro_f1 0.9161 val_macro_f1 0.7085\n",
      "Epoch 5 Batch 100 Loss 0.0376 micro_f1 0.9013 macro_f1 0.6942 val_micro_f1 0.9031 val_macro_f1 0.6507\n",
      "Epoch 5 Batch 150 Loss 0.0378 micro_f1 0.9222 macro_f1 0.6870 val_micro_f1 0.8980 val_macro_f1 0.6331\n",
      "Epoch 5 Batch 200 Loss 0.0376 micro_f1 0.9130 macro_f1 0.6757 val_micro_f1 0.9065 val_macro_f1 0.7072\n",
      "Epoch 5 Batch 250 Loss 0.0369 micro_f1 0.8699 macro_f1 0.6436 val_micro_f1 0.9200 val_macro_f1 0.6759\n",
      "Saving checkpoint for epoch 5 at /home/abner/Documents/PycharmProjects/Mark_the_KN/result/ckpt-1\n",
      "Epoch 5 Loss 0.0363 Accuracy 0.9879\n",
      "Time taken for 1 epoch: 211.6494174003601 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.0289 micro_f1 0.9053 macro_f1 0.5960 val_micro_f1 0.8750 val_macro_f1 0.6248\n",
      "Epoch 6 Batch 50 Loss 0.0375 micro_f1 0.9207 macro_f1 0.7048 val_micro_f1 0.9074 val_macro_f1 0.6818\n",
      "Epoch 6 Batch 100 Loss 0.0355 micro_f1 0.9096 macro_f1 0.7298 val_micro_f1 0.9114 val_macro_f1 0.6603\n",
      "Epoch 6 Batch 150 Loss 0.0350 micro_f1 0.9413 macro_f1 0.6505 val_micro_f1 0.9023 val_macro_f1 0.6666\n",
      "Epoch 6 Batch 200 Loss 0.0343 micro_f1 0.9275 macro_f1 0.7216 val_micro_f1 0.9133 val_macro_f1 0.6813\n",
      "Epoch 6 Batch 250 Loss 0.0336 micro_f1 0.9032 macro_f1 0.6844 val_micro_f1 0.9430 val_macro_f1 0.6923\n",
      "Epoch 6 Loss 0.0339 Accuracy 0.9891\n",
      "Time taken for 1 epoch: 214.4738049507141 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.0293 micro_f1 0.9303 macro_f1 0.6964 val_micro_f1 0.8889 val_macro_f1 0.6519\n",
      "Epoch 7 Batch 50 Loss 0.0348 micro_f1 0.9176 macro_f1 0.6765 val_micro_f1 0.9101 val_macro_f1 0.6629\n",
      "Epoch 7 Batch 100 Loss 0.0337 micro_f1 0.8825 macro_f1 0.6768 val_micro_f1 0.9409 val_macro_f1 0.7005\n",
      "Epoch 7 Batch 150 Loss 0.0341 micro_f1 0.9261 macro_f1 0.6649 val_micro_f1 0.9050 val_macro_f1 0.6642\n",
      "Epoch 7 Batch 200 Loss 0.0335 micro_f1 0.9239 macro_f1 0.7289 val_micro_f1 0.9132 val_macro_f1 0.6734\n",
      "Epoch 7 Batch 250 Loss 0.0330 micro_f1 0.9410 macro_f1 0.7405 val_micro_f1 0.9267 val_macro_f1 0.6528\n",
      "Epoch 7 Loss 0.0328 Accuracy 0.9899\n",
      "Time taken for 1 epoch: 207.8908200263977 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0252 micro_f1 0.9282 macro_f1 0.6824 val_micro_f1 0.8921 val_macro_f1 0.6542\n",
      "Epoch 8 Batch 50 Loss 0.0349 micro_f1 0.9305 macro_f1 0.7406 val_micro_f1 0.8967 val_macro_f1 0.6412\n",
      "Epoch 8 Batch 100 Loss 0.0338 micro_f1 0.9351 macro_f1 0.7364 val_micro_f1 0.9111 val_macro_f1 0.6436\n",
      "Epoch 8 Batch 150 Loss 0.0329 micro_f1 0.9179 macro_f1 0.6362 val_micro_f1 0.9201 val_macro_f1 0.6667\n",
      "Epoch 8 Batch 200 Loss 0.0327 micro_f1 0.9467 macro_f1 0.7813 val_micro_f1 0.9064 val_macro_f1 0.6695\n",
      "Epoch 8 Batch 250 Loss 0.0322 micro_f1 0.9093 macro_f1 0.6259 val_micro_f1 0.9193 val_macro_f1 0.7064\n",
      "Epoch 8 Loss 0.0320 Accuracy 0.9903\n",
      "Time taken for 1 epoch: 208.61389923095703 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0507 micro_f1 0.8875 macro_f1 0.6059 val_micro_f1 0.9044 val_macro_f1 0.7089\n",
      "Epoch 9 Batch 50 Loss 0.0370 micro_f1 0.9301 macro_f1 0.7264 val_micro_f1 0.9224 val_macro_f1 0.6831\n",
      "Epoch 9 Batch 100 Loss 0.0343 micro_f1 0.9223 macro_f1 0.6923 val_micro_f1 0.9222 val_macro_f1 0.7084\n",
      "Epoch 9 Batch 150 Loss 0.0336 micro_f1 0.9209 macro_f1 0.6581 val_micro_f1 0.9277 val_macro_f1 0.6627\n",
      "Epoch 9 Batch 200 Loss 0.0336 micro_f1 0.9412 macro_f1 0.7809 val_micro_f1 0.9372 val_macro_f1 0.7225\n",
      "Epoch 9 Batch 250 Loss 0.0329 micro_f1 0.9036 macro_f1 0.6174 val_micro_f1 0.9207 val_macro_f1 0.6895\n",
      "Epoch 9 Loss 0.0326 Accuracy 0.9904\n",
      "Time taken for 1 epoch: 207.8781087398529 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0265 micro_f1 0.9270 macro_f1 0.7102 val_micro_f1 0.9248 val_macro_f1 0.6990\n",
      "Epoch 10 Batch 50 Loss 0.0353 micro_f1 0.9135 macro_f1 0.7154 val_micro_f1 0.9439 val_macro_f1 0.7065\n",
      "Epoch 10 Batch 100 Loss 0.0349 micro_f1 0.8962 macro_f1 0.6945 val_micro_f1 0.9060 val_macro_f1 0.6627\n",
      "Epoch 10 Batch 150 Loss 0.0346 micro_f1 0.8912 macro_f1 0.6666 val_micro_f1 0.9136 val_macro_f1 0.6526\n",
      "Epoch 10 Batch 200 Loss 0.0342 micro_f1 0.9151 macro_f1 0.6934 val_micro_f1 0.9281 val_macro_f1 0.7084\n",
      "Epoch 10 Batch 250 Loss 0.0336 micro_f1 0.9217 macro_f1 0.6595 val_micro_f1 0.8929 val_macro_f1 0.5769\n",
      "Saving checkpoint for epoch 10 at /home/abner/Documents/PycharmProjects/Mark_the_KN/result/ckpt-2\n",
      "Epoch 10 Loss 0.0335 Accuracy 0.9903\n",
      "Time taken for 1 epoch: 208.06046271324158 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(params.epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    # inp -> portuguese, tar -> english\n",
    "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "        mic_f1, mac_f1 = train_step(inp, tar)\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            test_input, test_target = next(iter(test_dataset))\n",
    "            enc_padding_mask = create_padding_mask(test_input)\n",
    "            val_mic_f1, val_mac_f1 = predict(test_input, test_target, enc_padding_mask)\n",
    "\n",
    "            print(\n",
    "                'Epoch {} Batch {} Loss {:.4f} micro_f1 {:.4f} macro_f1 {:.4f} val_micro_f1 {:.4f} val_macro_f1 {:.4f}'.format(\n",
    "                    epoch + 1, batch, train_loss.result(), mic_f1, mac_f1, val_mic_f1, val_mac_f1))\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print('Saving checkpoint for epoch {} at {}'.format(epoch + 1,\n",
    "                                                            ckpt_save_path))\n",
    "\n",
    "    print('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1,\n",
    "                                                        train_loss.result(),\n",
    "                                                        train_accuracy.result()))\n",
    "\n",
    "    print('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp02",
   "language": "python",
   "name": "nlp02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
